{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww17460\viewh10080\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 == Kevin Kordish ==\
The program compiles fine and runs the test files my group used, although our test files were not very extensive. The implementation is similar to ours in the sense that the condition variables and mutexes are used in almost the same way. The way the task queue is structured is another difference from our implementation, but both implementations function the same way. We created node structs and made an task queue struct which contains the head in hopes that it made the code a little more clear. Your implementation just skips the extra struct and works the same way which I believe is just based on preference. You used in the global aggregate mutex in the way it was intended. Our group created a struct for all aggregate variables that each thread had a copy of, so at the end every thread did the calculations on the struct to come up with the correct output. This removed the need for the global aggregate mutex, but it may have increased execution time with the number of assignments and computations. I believe your implementation with the mutex is better. The code is easy to understand and follow. The comments are helpful and clear.\
\
Where both of our implementations run into a problem when running helgrind is at the while loop in the worker function. By not locking before the while loop, we create a potential data race as multiple threads can pass the conditions for the while loop before the variables used in the boolean statements are properly updated. This is a pretty easy fix though. You are also not freeing all allocs as when I run my test file using valgrind there are 886 allocs and only 882 frees. This should be another easy fix\'97 you need to free the memory allocated for the array of threads. Also, you do not do error checking on all pthread function calls, and in your cleanup you do not destroy the mutexes and condition variables. These are less important details though.\
\
==Brendan Pho==\
The program compiles fine. However, when running a test file with over 600 tasks, the only way it runs within a reasonable time (~30sec) is with 10 threads\'97sleeps commented out. Your implementation differs from ours in a lot of ways. The implementation of the queue is almost the same as ours apart from using a tail. The main difference is the number of mutexes used (we used 2, you used 3 (initialized a 4th but never used it\'97jobs_agg_mutex)) and the use of a semaphore. You used the mutex for the aggregate variables in the way it was intended as opposed to our implementation of a struct that help all the aggregate variables for each thread and did the computations at the end, eliminating the need for that mutex but at the cost of a potential increase in execution time. The use of an extra mutex for the condition variable is fine but unnecessary as you can use the queue mutex. The code is relatively easy to follow if you read the comments, however the variable name\'97time\'97 is a little confusing since computations are being done on it such as summation, checking if it\'92s odd, etc. I believe the main problem is the implementation of the semaphore as it is essentially running in serial until you sem_post on the number of threads to wake them all up once the task queue has received all of its tasks.\
\
There are no errors when running helgrind and valgrind. There are no race conditions because of the semaphore, and you free all the memory you allocate. However the program does not scale linearly and requires a large number of threads to run in a reasonable amount of time. With a few fixes it could work fine.}